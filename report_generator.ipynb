{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    api_key=GEMINI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama_32 = ChatGroq(\n",
    "    model=\"llama-3.1-70b-specdec\",\n",
    "    temperature=0,\n",
    "    api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llama = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/module_5.txt', 'r') as file:\n",
    "    module_5_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "    You are an expert in report generation. You are provided with summary of course videos of a module from the course\n",
    "    DevOps, DataOps and MLOps on Coursera. Your job is to provide a detailed academic report of this module. Provide the\n",
    "    report with minimum plagiarism possible. Write the report in an amateur manner as if you are a college student.\n",
    "    Do not output anything else other than the report. Provide the report in markdown format. Do not provide feedback.\n",
    "\n",
    "    Summary of Module: {module_5_content}\n",
    "    Report:\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# DevOps, DataOps and MLOps Module Report\\n=============================================\\n\\n## Introduction\\n---------------\\n\\nThis report provides an overview of the key concepts and takeaways from the DevOps, DataOps, and MLOps module on Coursera. The module focuses on the advantages of transitioning from Python to Rust, emphasizing Rust's modern features and efficiencies that can enhance programming practices.\\n\\n## Advantages of Rust\\n---------------------\\n\\nRust offers several advantages over Python, including:\\n\\n* **Performance and Energy Efficiency**: Rust is comparable to C and C++ in terms of performance and energy efficiency, making it ideal for cost-effective applications.\\n* **Concurrency and Safety**: Rust simplifies multi-threaded programming, overcoming Python's Global Interpreter Lock limitations, allowing for efficient use of system resources.\\n* **User-Friendly Delivery**: Rust allows for easy distribution of compiled binaries, making it accessible for non-technical users to run applications without complex setups.\\n\\n## Leveraging Copilot and GitHub\\n-----------------------------\\n\\nThe module highlights the integration of Copilot within the GitHub ecosystem, particularly for Python programmers transitioning to Rust. Using Copilot with Visual Studio Code and GitHub Codespaces enhances productivity and allows for rapid project setup and testing.\\n\\n## Building a Rust Project\\n-------------------------\\n\\nThe process of creating a new Rust project involves using templates and commands like `cargo new`, which simplifies project structure and dependency management. Copilot assists in generating code snippets and boilerplate, allowing developers to focus on logic and functionality while benefiting from a fast feedback loop during development.\\n\\n## Rust Packaging and Ecosystem\\n------------------------------\\n\\nRust's crate system allows for easy access to a growing number of libraries, making it simple to install and use various utilities for web development and other applications. The Rust ecosystem is experiencing rapid growth, potentially outpacing Python in terms of library adoption and usage.\\n\\n## Energy Efficiency Comparison\\n------------------------------\\n\\nStudies show that C and Rust are the most energy-efficient programming languages, performing equivalently in terms of energy usage and computational time. Python, while popular, uses approximately 70 times more energy and takes about 70 times longer for equivalent tasks compared to C and Rust.\\n\\n## Systems Programming and MLOps\\n-------------------------------\\n\\nA production-first mindset is crucial in MLOps, similar to systems programming challenges seen in operating systems like Linux. Performance is critical when training machine learning models, as it involves handling large datasets and computational resources efficiently. Rust offers significant performance advantages, being up to 70 times faster than Python for many operations, making it ideal for high-performance machine learning tasks.\\n\\n## Building a Unit Test for a Rust Project\\n-----------------------------------------\\n\\nThe module emphasizes the importance of testing in software development, highlighting the creation of a library and the process of writing unit tests for a Rust project. Utilizing a Makefile streamlines the testing process, allowing for easy execution of tests and linting.\\n\\n## Data Engineering with Rust\\n-----------------------------\\n\\nRust is identified as a systems programming language that excels in data engineering tasks, allowing for efficient memory usage and high performance, especially when handling large datasets. The ability to create multi-threaded applications in Rust can lead to significant performance improvements, making it suitable for tasks like deduplication and data transformation.\\n\\n## Benefits of Rust in MLOps\\n---------------------------\\n\\nRust offers significant performance advantages, being up to 70 times faster than Python for many operations, making it ideal for high-performance machine learning tasks. The language supports binary portability, allowing models to be packaged and deployed easily, which is crucial for production environments.\\n\\n## Deep Learning and Rust\\n-------------------------\\n\\nDeep learning technologies, such as large language models and text-to-image generation, are gaining popularity, with Rust offering significant performance advantages. Rust can be up to 70 times faster than Python for certain tasks, making it an excellent choice for high-performance research environments.\\n\\n## Deploying a Containerized Rust Microservice\\n--------------------------------------------\\n\\nThe module highlights the deployment of a containerized Rust microservice using Google Cloud Platform (GCP) and the continuous delivery process involved. The application utilizes a continuous delivery model where code changes in GitHub trigger builds in GCP's cloud build environment.\\n\\n## Conclusion\\n----------\\n\\nIn conclusion, the DevOps, DataOps, and MLOps module on Coursera provides a comprehensive overview of the advantages of transitioning from Python to Rust. Rust offers significant performance advantages, energy efficiency, and safety features, making it an ideal choice for systems programming, data engineering, and MLOps. The module highlights the importance of testing, continuous delivery, and deployment of containerized applications using GCP.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_result = llama.invoke(prompt)\n",
    "llama_result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DevOps, DataOps and MLOps Module Report\n",
       "=============================================\n",
       "\n",
       "## Introduction\n",
       "---------------\n",
       "\n",
       "This report provides an overview of the key concepts and takeaways from the DevOps, DataOps, and MLOps module on Coursera. The module focuses on the advantages of transitioning from Python to Rust, emphasizing Rust's modern features and efficiencies that can enhance programming practices.\n",
       "\n",
       "## Advantages of Rust\n",
       "---------------------\n",
       "\n",
       "Rust offers several advantages over Python, including:\n",
       "\n",
       "* **Performance and Energy Efficiency**: Rust is comparable to C and C++ in terms of performance and energy efficiency, making it ideal for cost-effective applications.\n",
       "* **Concurrency and Safety**: Rust simplifies multi-threaded programming, overcoming Python's Global Interpreter Lock limitations, allowing for efficient use of system resources.\n",
       "* **User-Friendly Delivery**: Rust allows for easy distribution of compiled binaries, making it accessible for non-technical users to run applications without complex setups.\n",
       "\n",
       "## Leveraging Copilot and GitHub\n",
       "-----------------------------\n",
       "\n",
       "The module highlights the integration of Copilot within the GitHub ecosystem, particularly for Python programmers transitioning to Rust. Using Copilot with Visual Studio Code and GitHub Codespaces enhances productivity and allows for rapid project setup and testing.\n",
       "\n",
       "## Building a Rust Project\n",
       "-------------------------\n",
       "\n",
       "The process of creating a new Rust project involves using templates and commands like `cargo new`, which simplifies project structure and dependency management. Copilot assists in generating code snippets and boilerplate, allowing developers to focus on logic and functionality while benefiting from a fast feedback loop during development.\n",
       "\n",
       "## Rust Packaging and Ecosystem\n",
       "------------------------------\n",
       "\n",
       "Rust's crate system allows for easy access to a growing number of libraries, making it simple to install and use various utilities for web development and other applications. The Rust ecosystem is experiencing rapid growth, potentially outpacing Python in terms of library adoption and usage.\n",
       "\n",
       "## Energy Efficiency Comparison\n",
       "------------------------------\n",
       "\n",
       "Studies show that C and Rust are the most energy-efficient programming languages, performing equivalently in terms of energy usage and computational time. Python, while popular, uses approximately 70 times more energy and takes about 70 times longer for equivalent tasks compared to C and Rust.\n",
       "\n",
       "## Systems Programming and MLOps\n",
       "-------------------------------\n",
       "\n",
       "A production-first mindset is crucial in MLOps, similar to systems programming challenges seen in operating systems like Linux. Performance is critical when training machine learning models, as it involves handling large datasets and computational resources efficiently. Rust offers significant performance advantages, being up to 70 times faster than Python for many operations, making it ideal for high-performance machine learning tasks.\n",
       "\n",
       "## Building a Unit Test for a Rust Project\n",
       "-----------------------------------------\n",
       "\n",
       "The module emphasizes the importance of testing in software development, highlighting the creation of a library and the process of writing unit tests for a Rust project. Utilizing a Makefile streamlines the testing process, allowing for easy execution of tests and linting.\n",
       "\n",
       "## Data Engineering with Rust\n",
       "-----------------------------\n",
       "\n",
       "Rust is identified as a systems programming language that excels in data engineering tasks, allowing for efficient memory usage and high performance, especially when handling large datasets. The ability to create multi-threaded applications in Rust can lead to significant performance improvements, making it suitable for tasks like deduplication and data transformation.\n",
       "\n",
       "## Benefits of Rust in MLOps\n",
       "---------------------------\n",
       "\n",
       "Rust offers significant performance advantages, being up to 70 times faster than Python for many operations, making it ideal for high-performance machine learning tasks. The language supports binary portability, allowing models to be packaged and deployed easily, which is crucial for production environments.\n",
       "\n",
       "## Deep Learning and Rust\n",
       "-------------------------\n",
       "\n",
       "Deep learning technologies, such as large language models and text-to-image generation, are gaining popularity, with Rust offering significant performance advantages. Rust can be up to 70 times faster than Python for certain tasks, making it an excellent choice for high-performance research environments.\n",
       "\n",
       "## Deploying a Containerized Rust Microservice\n",
       "--------------------------------------------\n",
       "\n",
       "The module highlights the deployment of a containerized Rust microservice using Google Cloud Platform (GCP) and the continuous delivery process involved. The application utilizes a continuous delivery model where code changes in GitHub trigger builds in GCP's cloud build environment.\n",
       "\n",
       "## Conclusion\n",
       "----------\n",
       "\n",
       "In conclusion, the DevOps, DataOps, and MLOps module on Coursera provides a comprehensive overview of the advantages of transitioning from Python to Rust. Rust offers significant performance advantages, energy efficiency, and safety features, making it an ideal choice for systems programming, data engineering, and MLOps. The module highlights the importance of testing, continuous delivery, and deployment of containerized applications using GCP."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(llama_result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Academic Report: DevOps, DataOps, and MLOps Module Summary\\n\\nThis report summarizes a Coursera module focusing on deploying containerized machine learning microservices.  The module is broken down into several key sections, each building upon the previous one.  My analysis focuses on the practical application and theoretical underpinnings presented.\\n\\n**Section 1: Containerization and Cloud Platforms for Microservices**\\n\\nThis introductory section establishes the core concept of containerization as a crucial element for efficient microservice deployment.  The module highlights the benefits of packaging code and runtime environments together, enabling seamless deployment across diverse cloud platforms.  Specific platforms mentioned include AWS App Runner, GCP Cloud Run, and Azure App Services, all of which support continuous delivery.  The integration with MLOps platforms is also introduced, emphasizing the use of API calls for model deployment and experiment tracking services for automated model selection.  This section provides a high-level overview, setting the stage for more detailed discussions in subsequent sections.  The key takeaway is the importance of containers for streamlining the continuous delivery process.\\n\\n**Section 2: Containerizing a Microservice with AWS ECR and App Runner**\\n\\nThis section delves into the practical aspects of containerization, focusing specifically on AWS services.  The process of creating a repository in AWS Elastic Container Registry (ECR), building a Docker image, and deploying it using AWS App Runner is explained in detail.  The steps involved in building, tagging, and pushing Docker images are clearly outlined.  The importance of testing the microservice locally before deployment is emphasized, highlighting best practices for development.  The section concludes by reinforcing the importance of understanding containerization and deployment strategies for effective MLOps.  The hands-on nature of this section is a significant strength, providing practical experience to complement the theoretical concepts.\\n\\n**Section 3: Continuous Delivery for Machine Learning Models**\\n\\nThis section expands on the continuous delivery workflow, emphasizing its application to machine learning models, including both traditional models and large language models (LLMs).  The integration of source control (e.g., GitHub) and a build system for automated image creation is discussed.  The use of a container registry and GPU serving endpoints is highlighted, emphasizing the self-contained nature of the containerized environment and its role in reducing dependency issues.  The section also explains how clients can invoke models using tools like Star Coder, illustrating the practical application of the continuous delivery system.  The focus on efficiency and reduced friction in the development process is a key takeaway.\\n\\n**Section 4: Building a Logistics Tool with Wikipedia API and NLP**\\n\\nThis section shifts focus to a practical application of the concepts learned.  It details the development of a logistics tool that utilizes the Wikipedia API for information gathering and natural language processing (NLP) for analysis.  The creation of a custom library (\"mylib\") for handling Wikipedia searches and summaries is described.  The section emphasizes the practical steps involved in building and testing the tool\\'s functionality, including functions for searching, summarizing, and extracting keywords.  This section demonstrates the application of programming skills and API integration within the broader context of MLOps.\\n\\n**Section 5: Comparison of Copilot and CodeWhisperer**\\n\\nThis section briefly compares two AI-assisted coding tools: GitHub Copilot and AWS CodeWhisperer.  The comparison focuses on user experience, integration with development environments, and overall functionality.  While both tools offer AI-assisted coding, the report notes that Copilot is generally perceived as more intuitive and faster, while CodeWhisperer, though offering high-quality suggestions, may have slower response times and formatting issues.  The differences in integration with GitHub and AWS environments are also highlighted.\\n\\n**Section 6: Transfer Learning in NLP**\\n\\nThis section introduces the concept of transfer learning, particularly in the context of NLP.  It contrasts transfer learning with traditional supervised learning, highlighting the advantages of fine-tuning pre-trained models to reduce the need for extensive data and computational resources.  The role of platforms like Hugging Face in facilitating the fine-tuning process is mentioned.  The section concludes by discussing the growing adoption of transfer learning and its potential future applications.\\n\\n**Section 7: Rapid Tool Building with OpenAI**\\n\\nThe final section focuses on building tools quickly using OpenAI\\'s capabilities.  It covers setting up a project structure, implementing a command-line interface (CLI), and organizing code into libraries.  The importance of continuous integration and testing is emphasized.  This section provides a practical example of leveraging OpenAI\\'s APIs for rapid development and demonstrates best practices for code organization and testing.\\n\\n\\n**Overall Assessment:**\\n\\nThe module effectively combines theoretical concepts with practical applications, providing a comprehensive introduction to deploying containerized machine learning microservices.  The hands-on examples and practical exercises are particularly valuable, allowing learners to solidify their understanding of the key concepts.  The inclusion of different tools and platforms broadens the scope and relevance of the material.  The comparison of AI-assisted coding tools and the introduction to transfer learning add depth and context to the module.  However, a more in-depth discussion of potential challenges and troubleshooting techniques would further enhance the learning experience.\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_result = gemini.invoke(prompt)\n",
    "gemini_result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Academic Report: DevOps, DataOps, and MLOps Module Summary\n",
       "\n",
       "This report summarizes a Coursera module focusing on deploying containerized machine learning microservices.  The module is broken down into several key sections, each building upon the previous one.  My analysis focuses on the practical application and theoretical underpinnings presented.\n",
       "\n",
       "**Section 1: Containerization and Cloud Platforms for Microservices**\n",
       "\n",
       "This introductory section establishes the core concept of containerization as a crucial element for efficient microservice deployment.  The module highlights the benefits of packaging code and runtime environments together, enabling seamless deployment across diverse cloud platforms.  Specific platforms mentioned include AWS App Runner, GCP Cloud Run, and Azure App Services, all of which support continuous delivery.  The integration with MLOps platforms is also introduced, emphasizing the use of API calls for model deployment and experiment tracking services for automated model selection.  This section provides a high-level overview, setting the stage for more detailed discussions in subsequent sections.  The key takeaway is the importance of containers for streamlining the continuous delivery process.\n",
       "\n",
       "**Section 2: Containerizing a Microservice with AWS ECR and App Runner**\n",
       "\n",
       "This section delves into the practical aspects of containerization, focusing specifically on AWS services.  The process of creating a repository in AWS Elastic Container Registry (ECR), building a Docker image, and deploying it using AWS App Runner is explained in detail.  The steps involved in building, tagging, and pushing Docker images are clearly outlined.  The importance of testing the microservice locally before deployment is emphasized, highlighting best practices for development.  The section concludes by reinforcing the importance of understanding containerization and deployment strategies for effective MLOps.  The hands-on nature of this section is a significant strength, providing practical experience to complement the theoretical concepts.\n",
       "\n",
       "**Section 3: Continuous Delivery for Machine Learning Models**\n",
       "\n",
       "This section expands on the continuous delivery workflow, emphasizing its application to machine learning models, including both traditional models and large language models (LLMs).  The integration of source control (e.g., GitHub) and a build system for automated image creation is discussed.  The use of a container registry and GPU serving endpoints is highlighted, emphasizing the self-contained nature of the containerized environment and its role in reducing dependency issues.  The section also explains how clients can invoke models using tools like Star Coder, illustrating the practical application of the continuous delivery system.  The focus on efficiency and reduced friction in the development process is a key takeaway.\n",
       "\n",
       "**Section 4: Building a Logistics Tool with Wikipedia API and NLP**\n",
       "\n",
       "This section shifts focus to a practical application of the concepts learned.  It details the development of a logistics tool that utilizes the Wikipedia API for information gathering and natural language processing (NLP) for analysis.  The creation of a custom library (\"mylib\") for handling Wikipedia searches and summaries is described.  The section emphasizes the practical steps involved in building and testing the tool's functionality, including functions for searching, summarizing, and extracting keywords.  This section demonstrates the application of programming skills and API integration within the broader context of MLOps.\n",
       "\n",
       "**Section 5: Comparison of Copilot and CodeWhisperer**\n",
       "\n",
       "This section briefly compares two AI-assisted coding tools: GitHub Copilot and AWS CodeWhisperer.  The comparison focuses on user experience, integration with development environments, and overall functionality.  While both tools offer AI-assisted coding, the report notes that Copilot is generally perceived as more intuitive and faster, while CodeWhisperer, though offering high-quality suggestions, may have slower response times and formatting issues.  The differences in integration with GitHub and AWS environments are also highlighted.\n",
       "\n",
       "**Section 6: Transfer Learning in NLP**\n",
       "\n",
       "This section introduces the concept of transfer learning, particularly in the context of NLP.  It contrasts transfer learning with traditional supervised learning, highlighting the advantages of fine-tuning pre-trained models to reduce the need for extensive data and computational resources.  The role of platforms like Hugging Face in facilitating the fine-tuning process is mentioned.  The section concludes by discussing the growing adoption of transfer learning and its potential future applications.\n",
       "\n",
       "**Section 7: Rapid Tool Building with OpenAI**\n",
       "\n",
       "The final section focuses on building tools quickly using OpenAI's capabilities.  It covers setting up a project structure, implementing a command-line interface (CLI), and organizing code into libraries.  The importance of continuous integration and testing is emphasized.  This section provides a practical example of leveraging OpenAI's APIs for rapid development and demonstrates best practices for code organization and testing.\n",
       "\n",
       "\n",
       "**Overall Assessment:**\n",
       "\n",
       "The module effectively combines theoretical concepts with practical applications, providing a comprehensive introduction to deploying containerized machine learning microservices.  The hands-on examples and practical exercises are particularly valuable, allowing learners to solidify their understanding of the key concepts.  The inclusion of different tools and platforms broadens the scope and relevance of the material.  The comparison of AI-assisted coding tools and the introduction to transfer learning add depth and context to the module.  However, a more in-depth discussion of potential challenges and troubleshooting techniques would further enhance the learning experience.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(gemini_result.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
